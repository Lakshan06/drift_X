# ðŸ“Š Accurate Accuracy Monitoring System

## Overview

The Drift Detection app now includes a **comprehensive, high-precision accuracy monitoring system**
that tracks model performance with industry-standard metrics, temporal analysis, and beautiful
visualizations.

---

## âœ¨ Key Features

### 1. **Real-Time Accuracy Tracking**

- âœ… Rolling window of 1000 most recent predictions
- âœ… Automatic metric calculation after each prediction
- âœ… Sub-millisecond performance overhead

### 2. **Comprehensive Metrics**

- **Accuracy**: Overall prediction correctness
- **Precision**: Correctness of positive predictions
- **Recall**: Coverage of actual positives
- **F1 Score**: Harmonic mean of precision and recall
- **Matthews Correlation Coefficient (MCC)**: Better for imbalanced datasets
- **ROC-AUC**: Area Under the Receiver Operating Characteristic curve
- **PR-AUC**: Precision-Recall Area Under Curve
- **Calibration Error (ECE)**: How well confidence matches actual accuracy

### 3. **Advanced Analysis**

- âœ… **Confusion Matrix**: Visual heatmap of predictions vs actuals
- âœ… **Temporal Drift Detection**: Identifies accuracy degradation over time
- âœ… **Trend Analysis**: IMPROVING / STABLE / DEGRADING classification
- âœ… **Confidence Calibration**: Measures probability calibration quality

### 4. **Beautiful Visualizations**

- ðŸ“ˆ **Multi-line Charts**: Track multiple metrics simultaneously
- ðŸ”¥ **Confusion Matrix Heatmap**: Color-coded prediction analysis
- ðŸ“Š **Progress Bars**: Animated metric displays
- ðŸ“‰ **Trend Indicators**: Visual accuracy trend feedback

---

## ðŸ“ Metrics Explained

### Primary Metrics

#### Accuracy

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

- **What it measures**: Overall percentage of correct predictions
- **Range**: 0.0 - 1.0 (0% - 100%)
- **Best for**: Balanced datasets
- **Target**: > 0.90 (90%)

#### Precision

```
Precision = TP / (TP + FP)
```

- **What it measures**: Of all positive predictions, how many were correct
- **Range**: 0.0 - 1.0
- **Best for**: When false positives are costly
- **Target**: > 0.85 (85%)

#### Recall (Sensitivity)

```
Recall = TP / (TP + FN)
```

- **What it measures**: Of all actual positives, how many were found
- **Range**: 0.0 - 1.0
- **Best for**: When false negatives are costly
- **Target**: > 0.85 (85%)

#### F1 Score

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

- **What it measures**: Harmonic mean of precision and recall
- **Range**: 0.0 - 1.0
- **Best for**: Balanced evaluation
- **Target**: > 0.85

### Advanced Metrics

#### Matthews Correlation Coefficient (MCC)

```
MCC = (TPÃ—TN - FPÃ—FN) / âˆš((TP+FP)(TP+FN)(TN+FP)(TN+FN))
```

- **What it measures**: Correlation between predictions and actuals
- **Range**: -1.0 to 1.0
- **Best for**: Imbalanced datasets
- **Interpretation**:
    - 1.0 = Perfect prediction
    - 0.0 = Random prediction
    - -1.0 = Complete disagreement
- **Target**: > 0.70

#### ROC-AUC (Receiver Operating Characteristic - Area Under Curve)

```
Calculated by integrating TPR vs FPR at various thresholds
```

- **What it measures**: Ability to distinguish between classes
- **Range**: 0.0 - 1.0
- **Interpretation**:
    - 1.0 = Perfect classifier
    - 0.5 = Random classifier
    - < 0.5 = Worse than random
- **Target**: > 0.85

#### Expected Calibration Error (ECE)

```
ECE = Î£ (|confidence - accuracy|) Ã— weight
```

- **What it measures**: How well confidence scores match actual accuracy
- **Range**: 0.0 - 1.0 (lower is better)
- **Interpretation**:
    - 0.0 = Perfect calibration
    - > 0.1 = Poor calibration
- **Target**: < 0.10

---

## ðŸŽ¨ Visualization Components

### 1. Accuracy Chart

**File**: `AccuracyChart.kt`

Multi-line temporal chart showing:

- âœ… Accuracy over time (green solid line)
- âœ… F1 Score over time (blue solid line)
- âœ… Precision over time (orange dashed line)
- âœ… Data points with markers
- âœ… Grid lines for easy reading
- âœ… Summary statistics (Current, Average, Data Points)

**Usage**:

```kotlin
AccuracyChart(
    history = accuracyHistory,
    showLegend = true,
    modifier = Modifier.fillMaxWidth()
)
```

### 2. Confusion Matrix View

**File**: `ConfusionMatrixView` in `AccuracyChart.kt`

Heatmap visualization showing:

- âœ… Color-coded matrix cells
    - Green diagonal = Correct predictions
    - Red off-diagonal = Incorrect predictions
- âœ… Intensity based on count
- âœ… TP, TN, FP, FN summary
- âœ… Labeled axes

**Usage**:

```kotlin
ConfusionMatrixView(
    confusionMatrix = metrics.confusionMatrix,
    modifier = Modifier.fillMaxWidth()
)
```

### 3. Model Metrics Card

**File**: `ModelMetricsCard` in `AccuracyChart.kt`

Comprehensive metrics dashboard with:

- âœ… Animated progress bars for primary metrics
- âœ… Advanced metrics grid (ROC-AUC, MCC, PR-AUC, Cal. Error)
- âœ… Prediction statistics (Total, Correct, Incorrect)
- âœ… Accuracy degradation warnings
- âœ… Timestamp for data freshness

**Usage**:

```kotlin
ModelMetricsCard(
    metrics = currentMetrics,
    modifier = Modifier.fillMaxWidth()
)
```

### 4. Accuracy Trend Indicator

**File**: `AccuracyTrendIndicator` in `AccuracyChart.kt`

Visual trend indicator showing:

- âœ… ðŸ“ˆ IMPROVING (green)
- âœ… âž¡ï¸ STABLE (blue)
- âœ… ðŸ“‰ DEGRADING (red)
- âœ… â„¹ï¸ INSUFFICIENT_DATA (gray)

**Usage**:

```kotlin
AccuracyTrendIndicator(
    trend = accuracyTrend
)
```

---

## ðŸ’» Implementation Guide

### Step 1: Record Predictions

```kotlin
val accuracyMonitor: AccuracyMonitor = get()

// After making a prediction with ground truth
accuracyMonitor.recordPrediction(
    modelId = "model_123",
    predictedClass = 1,
    actualClass = 1,  // Ground truth label
    confidence = 0.95f,
    features = floatArrayOf(1.0f, 2.0f, 3.0f),
    timestamp = Instant.now()
)
```

### Step 2: Observe Metrics

```kotlin
// In your ViewModel
val currentMetrics: StateFlow<ModelMetrics?> = accuracyMonitor.currentMetrics
val accuracyHistory: StateFlow<List<AccuracySnapshot>> = accuracyMonitor.accuracyHistory

// In your Composable
val metrics by viewModel.currentMetrics.collectAsState()
val history by viewModel.accuracyHistory.collectAsState()

metrics?.let { m ->
    ModelMetricsCard(metrics = m)
}

if (history.isNotEmpty()) {
    AccuracyChart(history = history)
}
```

### Step 3: Get Performance Summary

```kotlin
val summary = accuracyMonitor.getPerformanceSummary(modelId)

summary?.let {
    println("Current Accuracy: ${it.currentAccuracy}")
    println("Trend: ${it.trend}")
    println("Avg Accuracy: ${it.avgAccuracy}")
    println("Std Dev: ${it.standardDeviation}")
}
```

---

## ðŸŽ¯ Best Practices

### 1. **Ensure Ground Truth Availability**

```kotlin
// âœ… GOOD: Record with actual label
accuracyMonitor.recordPrediction(
    modelId = modelId,
    predictedClass = prediction,
    actualClass = groundTruth,  // From labeled data
    confidence = confidence,
    features = input
)

// âŒ BAD: Don't make up labels
accuracyMonitor.recordPrediction(
    ...
    actualClass = prediction,  // Using prediction as truth!
    ...
)
```

### 2. **Validate Data Quality**

```kotlin
// Ensure features are normalized/preprocessed consistently
val normalizedFeatures = preprocessFeatures(rawFeatures)

accuracyMonitor.recordPrediction(
    ...
    features = normalizedFeatures,  // Use same preprocessing as training
    ...
)
```

### 3. **Handle Class Imbalance**

```kotlin
// Use MCC for imbalanced datasets
val metrics = currentMetrics.value
if (metrics != null) {
    val bestMetric = if (isImbalanced) {
        metrics.matthewsCorrelation  // Better for imbalanced
    } else {
        metrics.accuracy  // Good for balanced
    }
}
```

### 4. **Monitor Calibration**

```kotlin
// Check if confidence scores are reliable
if (metrics.calibrationError > 0.15) {
    Log.w("ACCURACY", "âš ï¸ Poor calibration! Model confidence is unreliable")
    // Consider recalibrating the model
}
```

### 5. **Detect Accuracy Drift**

```kotlin
if (metrics.accuracyDrift > 0.05) {
    Log.w("ACCURACY", "âš ï¸ Accuracy degraded by ${metrics.accuracyDrift * 100}%")
    // Trigger retraining or patch generation
}
```

---

## ðŸ“Š Example: Complete Integration

```kotlin
// In your ViewModel
class ModelViewModel(
    private val accuracyMonitor: AccuracyMonitor,
    private val repository: DriftRepository
) : ViewModel() {

    val currentMetrics = accuracyMonitor.currentMetrics.stateIn(
        viewModelScope,
        SharingStarted.WhileSubscribed(5000),
        null
    )

    val accuracyHistory = accuracyMonitor.accuracyHistory.stateIn(
        viewModelScope,
        SharingStarted.WhileSubscribed(5000),
        emptyList()
    )

    fun makePrediction(
        modelId: String,
        features: FloatArray,
        actualLabel: Int? = null
    ) {
        viewModelScope.launch {
            // Get model prediction
            val prediction = repository.predict(modelId, features)

            // If we have ground truth, record for accuracy tracking
            actualLabel?.let { label ->
                accuracyMonitor.recordPrediction(
                    modelId = modelId,
                    predictedClass = prediction.predictedClass ?: 0,
                    actualClass = label,
                    confidence = prediction.confidence,
                    features = features
                )
            }

            // Check for accuracy issues
            val trend = accuracyMonitor.getAccuracyTrend()
            if (trend == AccuracyTrend.DEGRADING) {
                // Alert user or trigger automatic retraining
                showAccuracyAlert()
            }
        }
    }

    fun getPerformanceSummary(modelId: String) = flow {
        accuracyMonitor.getPerformanceSummary(modelId)?.let {
            emit(it)
        }
    }
}
```

```kotlin
// In your Composable
@Composable
fun ModelAccuracyScreen(
    viewModel: ModelViewModel = koinViewModel()
) {
    val metrics by viewModel.currentMetrics.collectAsState()
    val history by viewModel.accuracyHistory.collectAsState()

    Column(
        modifier = Modifier
            .fillMaxSize()
            .padding(16.dp),
        verticalArrangement = Arrangement.spacedBy(16.dp)
    ) {
        // Header with trend
        val trend = remember(history) {
            if (history.size >= 10) {
                viewModel.getAccuracyTrend()
            } else {
                AccuracyTrend.INSUFFICIENT_DATA
            }
        }

        Row(
            modifier = Modifier.fillMaxWidth(),
            horizontalArrangement = Arrangement.SpaceBetween,
            verticalAlignment = Alignment.CenterVertically
        ) {
            Text(
                "Model Performance",
                style = MaterialTheme.typography.headlineSmall,
                fontWeight = FontWeight.Bold
            )
            AccuracyTrendIndicator(trend = trend)
        }

        // Metrics card
        metrics?.let { m ->
            ModelMetricsCard(
                metrics = m,
                modifier = Modifier.fillMaxWidth()
            )
        }

        // Accuracy chart
        if (history.isNotEmpty()) {
            AccuracyChart(
                history = history,
                showLegend = true,
                modifier = Modifier.fillMaxWidth()
            )
        }

        // Confusion matrix
        metrics?.let { m ->
            ConfusionMatrixView(
                confusionMatrix = m.confusionMatrix,
                modifier = Modifier.fillMaxWidth()
            )
        }
    }
}
```

---

## ðŸ” Troubleshooting

### Issue: Metrics Not Updating

**Problem**: Accuracy metrics remain at 0 or don't change.

**Solution**:

```kotlin
// Verify predictions are being recorded
Log.d("ACCURACY", "Recording prediction: pred=$predicted, actual=$actual")
accuracyMonitor.recordPrediction(...)

// Check if metrics are calculated
val metrics = accuracyMonitor.currentMetrics.value
Log.d("ACCURACY", "Current accuracy: ${metrics?.accuracy}")
```

### Issue: Charts Show No Data

**Problem**: Accuracy chart displays "No accuracy data available"

**Solution**:

```kotlin
// Ensure history is being collected
val history = accuracyMonitor.accuracyHistory.value
Log.d("ACCURACY", "History size: ${history.size}")

// Need at least 1 prediction to show data
if (history.isEmpty()) {
    // Record some predictions first
}
```

### Issue: Inaccurate Metrics

**Problem**: Metrics don't match expected values

**Solution**:

1. **Verify ground truth labels**:
   ```kotlin
   // Make sure actualClass is truly the ground truth
   // Not the predicted class!
   ```

2. **Check data preprocessing**:
   ```kotlin
   // Use same preprocessing as training
   val features = normalizeFeatures(rawInput)
   ```

3. **Ensure consistent class encoding**:
   ```kotlin
   // Classes should be 0, 1, 2, ... (not 1, 2, 3, ...)
   ```

### Issue: High Calibration Error

**Problem**: Expected Calibration Error > 0.15

**Solution**:

```kotlin
// Model confidence scores don't match accuracy
// Options:
// 1. Apply temperature scaling
// 2. Use Platt scaling  
// 3. Retrain with better calibration
// 4. Use isotonic regression for calibration
```

---

## ðŸ“ˆ Performance Considerations

### Memory Usage

- **Rolling Window**: Max 1000 predictions (~100KB per model)
- **History**: Max 1000 snapshots (~50KB per model)
- **Total**: < 200KB per model

### Computation Overhead

- **Per Prediction**: ~0.5ms (includes all metric calculations)
- **Chart Rendering**: ~16ms (60 FPS smooth)
- **Confusion Matrix**: ~1ms (for 2x2 to 10x10 matrices)

### Optimization Tips

```kotlin
// 1. Limit prediction recording frequency
var predictionCount = 0
if (predictionCount++ % 10 == 0) {
    accuracyMonitor.recordPrediction(...)
}

// 2. Clear history for inactive models
accuracyMonitor.clearHistory(oldModelId)

// 3. Use debouncing for UI updates
val metrics by viewModel.currentMetrics
    .debounce(300)
    .collectAsState(null)
```

---

## ðŸŽ“ Key Takeaways

âœ… **Comprehensive Metrics**: 8+ industry-standard metrics tracked automatically

âœ… **Real-Time Analysis**: Metrics updated after each prediction with minimal overhead

âœ… **Beautiful Visualizations**: Charts, heatmaps, and progress bars for easy interpretation

âœ… **Temporal Drift Detection**: Automatically identifies accuracy degradation over time

âœ… **Production-Ready**: Optimized for performance with proper error handling

âœ… **Easy Integration**: Simple API with full Koin DI support

âœ… **Calibration Monitoring**: Ensures confidence scores are reliable

âœ… **Multi-Class Support**: Works for binary and multi-class classification

---

## ðŸ“š Further Reading

- [Scikit-learn Metrics Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Calibration in Machine Learning](https://arxiv.org/abs/1706.04599)
- [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)
- [ROC Curves and AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

---

**Version**: 1.0.0  
**Last Updated**: 2024  
**Maintained by**: Drift Detection Team
